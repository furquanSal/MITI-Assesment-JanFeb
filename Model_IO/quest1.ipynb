{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model I/O\n",
    "##### Question 1 - Multiple Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Name 7 monuments of India.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 1st LLM Model - Mistral/Gemma/Bloom/GPT2\n",
    "\n",
    "import requests\n",
    "\n",
    "'''\n",
    "You can use these models as well for the comparison\n",
    "API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom-560m\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openai-community/gpt2\"\n",
    "'''\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/gemma-7b-it\"     # Gemma Loaded\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": prompt,\n",
    "\t\"parameters\" : {\"max_length\":500, \"min_length\":200}\n",
    "})\n",
    "\n",
    "# output\n",
    "gemmaResponse = output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 2nd LLM Model - Llama 2\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:2002/v1\", api_key=\"not-needed\")\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"local-model\",\n",
    "  messages=[\n",
    "    {\"role\" : 'system', \"content\" : \"You give one line meaningful answers.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "lamaResponse = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the 3rd LLM's response as a reference for evaluation\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "import os\n",
    "\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY_PERSONAL\"))\n",
    "referenceResponse = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'])\n",
    "scores1 = scorer.score(gemmaResponse, referenceResponse) \n",
    "scores2 = scorer.score(lamaResponse, referenceResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores for Gemma: {'rougeL': Score(precision=0.6190476190476191, recall=0.4482758620689655, fmeasure=0.52)}\n",
      "ROUGE scores for Llama: {'rougeL': Score(precision=0.6666666666666666, recall=0.5, fmeasure=0.5714285714285715)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROUGE scores for Gemma: {scores1}\")\n",
    "print(f\"ROUGE scores for Llama: {scores2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only name 7 monuments of India.\n",
      "\n",
      "1. Taj Mahal\n",
      "2. Qutub Minar\n",
      "3. Red Fort\n",
      "4. Victoria Memorial\n",
      "5. Gateway of India\n",
      "6. Khajuraho Temple Complex\n",
      "7. Sanchi Stupa\n"
     ]
    }
   ],
   "source": [
    "print(gemmaResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Taj Mahal (Agra)\n",
      "2. Red Fort (Delhi)\n",
      "3. Qutub Minar (Delhi)\n",
      "4. Lotus Temple (Delhi)\n",
      "5. Hawa Mahal (Jaipur)\n",
      "6. Akshardham (Guwahati)\n",
      "7. Sun Temple (Modhera, Gujarat)\n"
     ]
    }
   ],
   "source": [
    "print(lamaResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Taj Mahal\n",
      "2. Red Fort\n",
      "3. Qutub Minar\n",
      "4. India Gate\n",
      "5. Gateway of India\n",
      "6. Charminar\n",
      "7. Lotus Temple\n"
     ]
    }
   ],
   "source": [
    "print(referenceResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The precision score measures the percentage of correct predictions made by the model. In this case, Gemma 7b has a precision score of 0.619, meaning that 61.9% of its predictions were correct. Llama 7B, on the other hand, has a higher precision score of 0.667, indicating that 66.7% of its predictions were correct.\n",
      "\n",
      "The recall score measures the percentage of relevant items that were correctly retrieved by the model. Gemma 7b has a recall score of 0.448, meaning that it retrieved 44.8% of the relevant items. Llama 7B has a slightly higher recall score of 0.5, indicating that it retrieved 50% of the relevant items.\n",
      "\n",
      "The F-measure score is a weighted average of the precision and recall scores. It takes into account both measures to provide a balanced evaluation of the model's performance. Gemma 7b has an F-measure score of 0.52, while Llama 7B has a higher score of 0.571.\n",
      "\n",
      "Overall, Llama 7B has slightly better scores in all three measures compared to Gemma 7b. This means that Llama 7B is better\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "systempl = \"You analyse precision, recall, fmeasure scores and explain to the user in easy understandable english.\"\n",
    "sysmsg=SystemMessagePromptTemplate.from_template(systempl)\n",
    "humtempl = \"\"\"\n",
    "Analyse and generate the reports of the following models' score. \n",
    "1. The model {firstModel} got the score of {scores1} \n",
    "2. The model {secondModel} got the score of {scores2}.\n",
    "Explain the each score how  it is calculated and compare both the models.\n",
    "\"\"\"\n",
    "hummsg = HumanMessagePromptTemplate.from_template(humtempl)\n",
    "chatprompt = ChatPromptTemplate.from_messages([sysmsg,hummsg])\n",
    "prompt = chatprompt.format_prompt(firstModel=\"Gemma 7b\", secondModel=\"Llama 7B\", scores1=scores1, scores2=scores2)\n",
    "result = llm.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< End Of Code >>>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
